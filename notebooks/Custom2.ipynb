{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve, CalibrationDisplay\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import pickle\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import notebook_util as n_util\n",
    "from notebook_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/ift6758a-a22-g3-projet/custom-models/d4e108382dd04f3dbce68a425a4c6486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import comet_ml at the top of your file\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"UGYDiy3HENiE7Y3dqoMAVIgG2\",\n",
    "    project_name=\"custom-models\",\n",
    "    workspace=\"ift6758a-a22-g3-projet\",\n",
    ")\n",
    "\n",
    "# Report multiple hyperparameters using a dictionary:\n",
    "hyper_params = {\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"batch_size\": 100,\n",
    "    \"num_epochs\": 25,\n",
    "    \"momentum\": 0.5,\n",
    "}\n",
    "experiment.log_parameters(hyper_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bei Ning\\Documents\\GitHub\\ift6758-a22-g3-projet\\notebooks\\notebook_util.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['emptyNet'] = df_filtered['emptyNet'].fillna(0)\n",
      "c:\\Users\\Bei Ning\\Documents\\GitHub\\ift6758-a22-g3-projet\\notebooks\\notebook_util.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['strength'] = df_filtered['strength'].fillna('Even')\n"
     ]
    }
   ],
   "source": [
    "df_filtered = prep_data()\n",
    "\n",
    "df_filtered = prep_dummie(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206814, 36)\n",
      "(101864, 36)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df_filtered, test_size=0.33, random_state=42)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[n_util.feature]#.to_numpy().reshape(-1, columns_count)\n",
    "\n",
    "y_train = train['isGoal']#.to_numpy()\n",
    "\n",
    "x_train, y_train = RandomOverSampler().fit_resample(x_train, y_train)\n",
    "\n",
    "x_test = test[n_util.feature]#.to_numpy().reshape(-1, columns_count)\n",
    "\n",
    "y_test = test['isGoal']#.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(35, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = hyper_params[\"batch_size\"]\n",
    "num_epochs = hyper_params[\"num_epochs\"]\n",
    "learning_rate = hyper_params[\"learning_rate\"]\n",
    "momentum = hyper_params[\"momentum\"]\n",
    "batch_no = len(x_train) // batch_size\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_t = torch.tensor(x_train.to_numpy(dtype=np.float32), dtype = torch.float32)\n",
    "y_train_t = torch.tensor(y_train.values, dtype = torch.float32)\n",
    "\n",
    "x_test_t = torch.tensor(x_test.to_numpy(dtype=np.float32), dtype = torch.float32)\n",
    "y_test_t = torch.tensor(y_test.values, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Sum:  tensor(3.3487, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6904, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6888, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6872, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6853, grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      "Loss Sum:  tensor(0.6830, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6807, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6781, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6763, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6737, grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      "Loss Sum:  tensor(0.6709, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6672, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6641, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6621, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6601, grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      "Loss Sum:  tensor(0.6584, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6567, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6554, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6536, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6526, grad_fn=<DivBackward0>)\n",
      "Epoch 21\n",
      "Loss Sum:  tensor(0.6514, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6493, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6476, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6463, grad_fn=<DivBackward0>)\n",
      "Loss Sum:  tensor(0.6442, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch {}'.format(epoch+1))\n",
    "    x_train_t, y_train_t = shuffle(x_train_t, y_train_t)\n",
    "    # Mini batch learning\n",
    "    loss_sum = 0\n",
    "    for i in range(batch_no):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        x_var = Variable(torch.FloatTensor(x_train_t[start:end]))\n",
    "        y_var = Variable(torch.FloatTensor(y_train_t[start:end]))\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        ypred_var = net(x_var)\n",
    "        loss =criterion(ypred_var, y_var[:,None])\n",
    "        loss.backward()\n",
    "        loss_sum = loss_sum + loss\n",
    "        #print(loss)\n",
    "        optimizer.step()\n",
    "    print('Loss Sum: ', loss_sum/batch_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Right 59436\n",
      "Accuracy 0.58\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_var = Variable(torch.FloatTensor(x_test_t), requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    result = net(test_var)\n",
    "values = torch.round(result[:, 0])\n",
    "\n",
    "num_right = np.sum(values.data.numpy().astype(int) == y_test)\n",
    "print('Num Right', num_right)\n",
    "accuracy = num_right / len(y_test_t)\n",
    "print('Accuracy {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bei Ning\\AppData\\Local\\Temp\\ipykernel_38408\\1599096580.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred_m = torch.tensor(values).to(torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[52970,  2744],\n",
       "        [39684,  6466]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "\n",
    "target_m = torch.tensor(y_test.to_numpy()).to(torch.int)\n",
    "pred_m = torch.tensor(values).to(torch.int)\n",
    "\n",
    "confmat = ConfusionMatrix(num_classes=2)\n",
    "confmat(target_m, pred_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Pytorch\n",
      "0.2335982620716095\n",
      "F1 Macro\n",
      "0.47381681870231745\n"
     ]
    }
   ],
   "source": [
    "metric = BinaryF1Score()\n",
    "print('F1 Pytorch')\n",
    "f1 = metric(result[:, 0], target_m).item()\n",
    "print(f1)\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(target_m.numpy(), values.numpy().astype(int), average='macro')\n",
    "print('F1 Macro')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/ift6758a-a22-g3-projet/custom-models/d4e108382dd04f3dbce68a425a4c6486\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     accuracy    : 0.5834838608340532\n",
      "COMET INFO:     f1          : 0.47381681870231745\n",
      "COMET INFO:     loss [9388] : (0.5503088235855103, 58.038204193115234)\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size    : 100\n",
      "COMET INFO:     learning_rate : 0.0003\n",
      "COMET INFO:     momentum      : 0.5\n",
      "COMET INFO:     num_epochs    : 25\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     conda-environment-definition : 1\n",
      "COMET INFO:     conda-info                   : 1\n",
      "COMET INFO:     conda-specification          : 1\n",
      "COMET INFO:     environment details          : 1\n",
      "COMET INFO:     filename                     : 1\n",
      "COMET INFO:     git metadata                 : 1\n",
      "COMET INFO:     git-patch (uncompressed)     : 1 (330.55 KB)\n",
      "COMET INFO:     installed packages           : 1\n",
      "COMET INFO:     model graph                  : 1\n",
      "COMET INFO:     model-element                : 1 (588.59 KB)\n",
      "COMET INFO:     notebook                     : 1\n",
      "COMET INFO:     source_code                  : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(net, open(\"../models/MLP2.sav\", 'wb'))\n",
    "experiment.log_model(\"MLP2\", \"../models/MLP2.sav\")\n",
    "experiment.log_metric(\"f1\", f1)\n",
    "experiment.log_metric(\"accuracy\", accuracy)\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f5583cf1d9466b5c27e75c89cc6b383bed5736d6b16c51c8074d8690011a952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
